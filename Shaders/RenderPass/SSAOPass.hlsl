// clang-format off
#include "RootSignature/BindlessRS.hlsli"
#include "ShaderInterlop/ConstantBuffers.hlsli"
#include "ShaderInterlop/RenderResources.hlsli"

struct VSOutput
{
    float4 position : SV_Position;
    float2 textureCoord : Texture_Coord;
};

ConstantBuffer<interlop::SSAORenderResources> renderResources : register(b0);

[RootSignature(BindlessRootSignature)]
VSOutput VsMain(uint vertexID : SV_VertexID)
{
    static const float3 VERTEX_POSITIONS[3] = {float3(-1.0f, 1.0f, 0.0f), float3(3.0f, 1.0f, 0.0f),
                                               float3(-1.0f, -3.0f, 0.0f)};

    VSOutput output;
    output.position = float4(VERTEX_POSITIONS[vertexID], 1.0f);
    output.textureCoord = output.position.xy * float2(0.5f, -0.5f) + float2(0.5f, 0.5f);
    return output;
}

float PsMain(VSOutput input) : SV_Target
{
    Texture2D<float4> positionTexture = ResourceDescriptorHeap[renderResources.positionTextureIndex];
    Texture2D<float4> normalTexture = ResourceDescriptorHeap[renderResources.normalTextureIndex];
    Texture2D<float2> randomRotationTexture = ResourceDescriptorHeap[renderResources.randomRotationTextureIndex];
    
    // The plan in the future is to reconstruct the view space position using the input.texturecoord and the depth texture. This is why
    // in the SandBox code, I have setup depth transitions to SRV, and the depth texture can be read from this texture.
    // For reference code (for getting view space coords from depth buffer), see here : https://therealmjp.github.io/posts/reconstructing-position-from-depth-continued/
    Texture2D<float> depthTexture = ResourceDescriptorHeap[renderResources.depthTextureIndex];

    ConstantBuffer<interlop::SceneBuffer> sceneBuffer = ResourceDescriptorHeap[renderResources.sceneBufferIndex];
    ConstantBuffer<interlop::SSAOBuffer> ssaoBuffer = ResourceDescriptorHeap[renderResources.ssaoBufferIndex];
    
    const float2 noiseScale = float2((float)ssaoBuffer.screenWidth / ssaoBuffer.noiseTextureWidth, (float)ssaoBuffer.screenHeight / ssaoBuffer.noiseTextureHeight);
    const float3 randomVector = normalize(float3(randomRotationTexture.Sample(pointWrapSampler, input.textureCoord * noiseScale).xy, 0.0f));

    const float3 viewSpaceNormal = normalize(normalTexture.Sample(linearWrapSampler, input.textureCoord).xyz);
    const float3 viewSpacePosition = positionTexture.Sample(linearWrapSampler, input.textureCoord).xyz;
    
    const float currentPixelDepth = positionTexture.Sample(pointClampSampler, input.textureCoord).z;

    // The tangent is generated by taking the random vector and normal into account and using the Gramm Schmidt Process.
    // The Tanget will not be perpendicular to the geometry surface.

    const float3 tangent  = normalize(randomVector - viewSpaceNormal * dot(randomVector, viewSpaceNormal));
    const float3 biTangent = normalize(cross(viewSpaceNormal, tangent));

    const float3x3 tbnMatrix = float3x3(tangent, biTangent, viewSpaceNormal);

    float occlusion = 0.0f;

    for (uint i = 0; i < ssaoBuffer.sampleVectorCount; ++i)
    {
        const float3 viewSpaceSampleVector = mul(ssaoBuffer.sampleVectors[i].xyz, tbnMatrix);
        float4 samplePosition = float4(viewSpacePosition + viewSpaceSampleVector * ssaoBuffer.radius, 1.0f); // Basically placing the normal oriented hemisphere sampled position over the corresponding view space position.

        // Transforming sample position to clip space.
        samplePosition = mul(samplePosition, sceneBuffer.projectionMatrix);
        samplePosition.xy /= samplePosition.w;

        // Conversion from -1..1 to 0..1.
        samplePosition.xy = samplePosition.xy * float2(0.5f, -0.5f) + float2(0.5f, 0.5f);

        const float sampleDepth = positionTexture.Sample(pointClampSampler, samplePosition.xy).z;

        const float rangeCheck = smoothstep(0.0f, 1.0f, ssaoBuffer.radius / abs(sampleDepth - currentPixelDepth));

        occlusion += rangeCheck * ((samplePosition.z + ssaoBuffer.bias) <= sampleDepth ? 0.0f : 1.0f);
    }

    return 1.0f - (occlusion / 64.0f);
}